#+Title: CFEngine Enteprise HA Environment


* Bringing up the environment

1) Download the CFEngine 3.12.0 hub RPM.

   #+BEGIN_SRC shell
     wget "https://cfengine-package-repos.s3.amazonaws.com/enterprise/Enterprise-3.12.0/hub/redhat_6_x86_64/cfengine-nova-hub-3.12.0-1.x86_64.rpm"
   #+END_SRC

2) Initialize the environment.

   #+BEGIN_SRC shell
     vagrant up node2 && vagrant up node1
   #+END_SRC

   Note: Ordering is important here
   because libvirt will bring up the nodes in parallel and node2 must be up
   before ~pcs cluster auth~.

3) Finish cluster setup manually (if /provisioning/ failed).

   #+BEGIN_EXAMPLE
     ==> node1: Restarting pcsd on the nodes in order to reload the certificates...
     ==> node1: node1: Success
     ==> node1: node2: Success
     ==> node1: ++ pcs cluster start --all
     ==> node1: node1: Unable to connect to node1 (Connection error)
     ==> node1: node2: Unable to connect to node2 (Connection error)
     ==> node1: Error: unable to start all nodes
     ==> node1: node1: Unable to connect to node1 (Connection error)
     ==> node1: node2: Unable to connect to node2 (Connection error)
     The SSH command responded with a non-zero exit status. Vagrant
     assumes that this means the command failed. The output for this command
     should be in the log above. Please read the output to determine what
     went wrong.
   #+END_EXAMPLE

   On Nicks workstation, vagrant errors out after ~pcs cluster start --all~. If
   that happens, *make sure to repeat the failed command and all commands that
   follow the failed one manually*. *All the /provision/ steps are critical.*

* Start PostgreSQL on *node1*

   #+BEGIN_SRC shell
     pushd /tmp; su cfpostgres -c "/var/cfengine/bin/pg_ctl -w -D /var/cfengine/state/pg/data -l /var/log/postgresql.log start"; popd
   #+END_SRC

* Configure PostgreSQL on *node2*

1) Purge the PostgreSQL data directory and initialize the database from the
   state of node1.

   #+BEGIN_SRC shell
     rm -rf /var/cfengine/state/pg/data/*
     pushd /tmp; su cfpostgres -c "cd /tmp && /var/cfengine/bin/pg_basebackup -h node1-pg -U cfpostgres -D /var/cfengine/state/pg/data -X stream -P"; popd
   #+END_SRC

2) Set the recovery mode for PostgreSQL.

   #+BEGIN_SRC shell
     cp /vagrant/recovery.conf /var/cfengine/state/pg/data/recovery.conf
     chown --reference /var/cfengine/state/pg/data/postgresql.conf /var/cfengine/state/pg/data/recovery.conf
   #+END_SRC

* Start and check PostgreSQL then stop it

1) Start PostgreSQL by the following command *on the node2*.

   #+BEGIN_SRC shell
     pushd /tmp; su cfpostgres -c "/var/cfengine/bin/pg_ctl -D /var/cfengine/state/pg/data -l /var/log/postgresql.log start"; popd
   #+END_SRC

2) Check that *node2* is working as a hot standby.

   #+BEGIN_SRC shell
     /var/cfengine/bin/psql cfdb -c "SELECT pg_is_in_recovery();"  # should give just 't'
   #+END_SRC

3) Check that *node1* is replicating.

   #+BEGIN_SRC shell
     /var/cfengine/bin/psql -x cfdb -c "SELECT * FROM pg_stat_replication;"  # should give state for replication to node2
   #+END_SRC

   Expected output:

   #+BEGIN_EXAMPLE
     -[ RECORD 1 ]----+------------------------------
     pid              | 30731
     usesysid         | 10
     usename          | cfpostgres
     application_name | node2
     client_addr      | 192.168.10.11
     client_hostname  | node2-pg
     client_port      | 60036
     backend_start    | 2018-10-05 19:06:56.599435+00
     backend_xmin     |
     state            | streaming
     sent_lsn         | 0/301F098
     write_lsn        | 0/301F098
     flush_lsn        | 0/301F098
     replay_lsn       | 0/301F098
     write_lag        | 00:00:00.000377
     flush_lag        | 00:00:00.000645
     replay_lag       | 00:00:00.00096
     sync_priority    | 0
     sync_state       | async
   #+END_EXAMPLE

4) Stop PostgreSQL *on both nodes*.

   #+BEGIN_SRC shell
     pushd /tmp; su cfpostgres -c "/var/cfengine/bin/pg_ctl -D /var/cfengine/state/pg/data -l /var/log/postgresql.log stop"; popd
   #+END_SRC

* Configure the PostgreSQL cluster resource

1) Create the base resource.

   #+BEGIN_SRC shell
     pcs resource create cfpgsql pgsql  \
       pgctl="/var/cfengine/bin/pg_ctl" \
       psql="/var/cfengine/bin/psql"    \
       pgdata="/var/cfengine/state/pg/data" \
       pgdb="cfdb" pgdba="cfpostgres" repuser="cfpostgres" \
       tmpdir="/var/cfengine/state/pg/tmp" \
       rep_mode="async" node_list="node1 node2" \
       primary_conninfo_opt="keepalives_idle=60 keepalives_interval=5 keepalives_count=5" \
       master_ip="192.168.10.100" restart_on_promote="true" \
       logfile="/var/log/postgresql.log" \
       config="/var/cfengine/state/pg/data/postgresql.conf" \
       check_wal_receiver=true restore_command="cp /var/cfengine/state/pg/data/pg_arch/%f %p" \
       op monitor timeout="60s" interval="3s" on-fail="restart" role="Master" \
       op monitor timeout="60s" interval="4s" on-fail="restart" --disable
   #+END_SRC

2) Transform/wrap the resource into a Master/Slave resource.

   #+BEGIN_SRC shell
     pcs resource master mscfpgsql cfpgsql master-max=1 master-node-max=1 clone-max=2 clone-node-max=1 notify=true
   #+END_SRC

3) Set the constraints for the resource.

   #+BEGIN_SRC shell
     pcs constraint colocation add cfengine with Master mscfpgsql INFINITY
     pcs constraint order promote mscfpgsql then start cfengine symmetrical=false score=INFINITY
     pcs constraint order demote mscfpgsql then stop cfengine symmetrical=false score=0
     pcs constraint location mscfpgsql prefers node1
   #+END_SRC

4) Enable the resource.

   #+BEGIN_SRC shell
     pcs resource enable mscfpgsql --wait=30
   #+END_SRC

5) Check the constraints configuration.

   #+BEGIN_SRC shell
     pcs constraint
   #+END_SRC

   Should give:

   #+BEGIN_SRC
     Location Constraints:
       Resource: mscfpgsql
         Enabled on: node1 (score:INFINITY)
     Ordering Constraints:
       promote mscfpgsql then start cfengine (score:INFINITY) (non-symmetrical)
       demote mscfpgsql then stop cfengine (score:0) (non-symmetrical)
     Colocation Constraints:
       cfengine with mscfpgsql (score:INFINITY) (rsc-role:Started) (with-rsc-role:Master)
     Ticket Constraints:
   #+END_SRC

6) Check the cluster status.

   #+BEGIN_SRC shell
     crm_mon -Afr1
   #+END_SRC

   Should give:

   #+BEGIN_SRC
     Stack: cman
     Current DC: node2 (version 1.1.18-3.el6-bfe4e80420) - partition with quorum
     Last updated: Fri Oct  5 10:03:01 2018
     Last change: Fri Oct  5 09:58:49 2018 by root via crm_attribute on node1

     2 nodes configured
     3 resources configured

     Online: [ node1 node2 ]

     Full list of resources:

      Resource Group: cfengine
          cfvirtip	(ocf::heartbeat:IPaddr2):	Started node1
      Master/Slave Set: mscfpgsql [cfpgsql]
          Masters: [ node1 ]
          Slaves: [ node2 ]

     Node Attributes:
     * Node node1:
         + cfpgsql-data-status             	: LATEST    
         + cfpgsql-master-baseline         	: 0000000004000090
         + cfpgsql-receiver-status         	: ERROR     
         + cfpgsql-status                  	: PRI       
         + master-cfpgsql                  	: 1000      
     * Node node2:
         + cfpgsql-data-status             	: STREAMING|ASYNC
         + cfpgsql-receiver-status         	: normal    
         + cfpgsql-status                  	: HS:async  
         + master-cfpgsql                  	: 100       

     Migration Summary:
     * Node node2:
     * Node node1:
   #+END_SRC

   *If the output doesn't look like the example above (one Master, one Slave,
   one =PRI= status, one =HS:async= or =HS:alone= status), try:*

   #+BEGIN_SRC shell
     pcs cluster stop --all && pcs cluster start --all
   #+END_SRC

   and check the status again.

* Check that PostgreSQL HA works

1) Take the *node1* down.

   #+BEGIN_SRC shell
     vagrant halt node1
   #+END_SRC

2) Check that the migration happened and *node2* is now the active (master) node.

   #+BEGIN_SRC shell
     crm_mon -Afr1
   #+END_SRC

   Should give:

   #+BEGIN_SRC
     Stack: cman
     Current DC: node2 (version 1.1.18-3.el6-bfe4e80420) - partition with quorum
     Last updated: Fri Oct  5 10:04:21 2018
     Last change: Fri Oct  5 10:03:48 2018 by root via crm_attribute on node2

     2 nodes configured
     3 resources configured

     Online: [ node2 ]
     OFFLINE: [ node1 ]

     Full list of resources:

      Resource Group: cfengine
          cfvirtip	(ocf::heartbeat:IPaddr2):	Started node2
      Master/Slave Set: mscfpgsql [cfpgsql]
          Masters: [ node2 ]
          Stopped: [ node1 ]

     Node Attributes:
     * Node node2:
         + cfpgsql-data-status             	: LATEST    
         + cfpgsql-master-baseline         	: 0000000005000090
         + cfpgsql-receiver-status         	: ERROR     
         + cfpgsql-status                  	: PRI       
         + master-cfpgsql                  	: 1000      

     Migration Summary:
     * Node node2:
   #+END_SRC

3) Start *node1* again.

   #+BEGIN_SRC shell
     vagrant up node1
   #+END_SRC

4) Check the cluster status.

   #+BEGIN_SRC shell
     crm_mon -Afr1
   #+END_SRC

   Should give something like this (note the /DISCONNECT/ status on *node1*):

   #+BEGIN_SRC shell
     Stack: cman
     Current DC: node2 (version 1.1.18-3.el6-bfe4e80420) - partition with quorum
     Last updated: Fri Oct  5 10:05:51 2018
     Last change: Fri Oct  5 10:03:48 2018 by root via crm_attribute on node2

     2 nodes configured
     3 resources configured

     Online: [ node1 node2 ]

     Full list of resources:

      Resource Group: cfengine
          cfvirtip	(ocf::heartbeat:IPaddr2):	Started node2
      Master/Slave Set: mscfpgsql [cfpgsql]
          Masters: [ node2 ]
          Stopped: [ node1 ]

     Node Attributes:
     * Node node1:
         + cfpgsql-data-status             	: DISCONNECT
         + cfpgsql-status                  	: STOP      
         + master-cfpgsql                  	: -INFINITY 
     * Node node2:
         + cfpgsql-data-status             	: LATEST    
         + cfpgsql-master-baseline         	: 0000000005000090
         + cfpgsql-receiver-status         	: ERROR     
         + cfpgsql-status                  	: PRI       
         + master-cfpgsql                  	: 1000      

     Migration Summary:
     * Node node2:
     * Node node1:
        cfpgsql: migration-threshold=1 fail-count=1000000 last-failure='Fri Oct  5 10:05:33 2018'

     Failed Actions:
     * cfpgsql_start_0 on node1 'unknown error' (1): call=15, status=complete, exitreason='',
         last-rc-change='Fri Oct  5 10:05:33 2018', queued=0ms, exec=121ms
   #+END_SRC

5) Check that it is the concistency lock causing the failure.

   #+BEGIN_SRC shell
     pcs resource debug-start cfpgsql
   #+END_SRC

   Should give:

   #+BEGIN_SRC
     Operation start for cfpgsql:0 (ocf:heartbeat:pgsql) returned: 'unknown error' (1)
      >  stderr: ERROR: My data may be inconsistent. You have to remove /var/cfengine/state/pg/tmp/PGSQL.lock file to force start.
   #+END_SRC

6) Remove the lock and start the resource.

   #+BEGIN_SRC shell
     rm -f /var/cfengine/state/pg/tmp/PGSQL.lock
     pcs resource debug-start cfpgsql
   #+END_SRC

7) Check the cluster status.

   #+BEGIN_SRC shell
     crm_mon -Afr1
   #+END_SRC

   Should give something like this (i.e. states swapped between node1 and node2 compared to the original state):

   #+BEGIN_SRC shell
     Stack: cman
     Current DC: node2 (version 1.1.18-3.el6-bfe4e80420) - partition with quorum
     Last updated: Fri Oct  5 12:07:38 2018
     Last change: Fri Oct  5 10:09:42 2018 by root via crm_attribute on node2

     2 nodes configured
     3 resources configured

     Online: [ node1 node2 ]

     Full list of resources:

      Resource Group: cfengine
          cfvirtip	(ocf::heartbeat:IPaddr2):	Started node2
      Master/Slave Set: mscfpgsql [cfpgsql]
          Masters: [ node2 ]
          Stopped: [ node1 ]

     Node Attributes:
     * Node node1:
         + cfpgsql-data-status             	: STREAMING|ASYNC
         + cfpgsql-receiver-status         	: normal    
         + cfpgsql-status                  	: HS:async  
         + master-cfpgsql                  	: 100       
     * Node node2:
         + cfpgsql-data-status             	: LATEST    
         + cfpgsql-master-baseline         	: 0000000005000090
         + cfpgsql-receiver-status         	: ERROR     
         + cfpgsql-status                  	: PRI       
         + master-cfpgsql                  	: 1000      

     Migration Summary:
     * Node node2:
     * Node node1:
        cfpgsql: migration-threshold=1 fail-count=1000000 last-failure='Fri Oct  5 10:05:33 2018'

     Failed Actions:
     * cfpgsql_start_0 on node1 'unknown error' (1): call=15, status=complete, exitreason='',
         last-rc-change='Fri Oct  5 10:05:33 2018', queued=0ms, exec=121ms
   #+END_SRC
* Configure CFEngine HA

1) Bootstrap *both nodes to node1*.

   #+BEGIN_SRC shell
     cf-agent --bootstrap node1-pg
   #+END_SRC

2) Bootstrap the *node2* to itself.

   #+BEGIN_SRC shell
     cf-agent --bootstrap node2-pg
   #+END_SRC

3) Write the HA config JSON *on both nodes*.

   #+BEGIN_SRC shell
     cat <<EOF > /var/cfengine/masterfiles/cfe_internal/enterprise/ha/ha_info.json
{
  "192.168.100.10":
    {
     "sha": "c14a17325b9a1bdb0417662806f579e4187247317a9e1739fce772992ee422f6",
     "internal_ip": "192.168.100.10"
    },
  "192.168.100.11":
    {
     "sha": "b492eb4b59541c02a13bd52efe17c6a720e8a43b7c8f8803f3fc85dee7951e4f",
     "internal_ip": "192.168.100.11"
    }
}
EOF
   #+END_SRC

4) Get the host keys and replace them in the config JSON *on both nodes*.

   #+BEGIN_SRC shell
     cf-key -s
   #+END_SRC

   #+BEGIN_SRC shell
     sed -i 's/c14a17325b9a1bdb0417662806f579e4187247317a9e1739fce772992ee422f6/KEY_OF_THE_NODE1/' /var/cfengine/masterfiles/cfe_internal/enterprise/ha/ha_info.json
     sed -i 's/b492eb4b59541c02a13bd52efe17c6a720e8a43b7c8f8803f3fc85dee7951e4f/KEY_OF_THE_NODE2/' /var/cfengine/masterfiles/cfe_internal/enterprise/ha/ha_info.json
   #+END_SRC

5) Enable HA *on both nodes* in the =/var/cfengine/masterfiles/controls/def.cf=
   file by uncommenting the following line and commenting out the line above it.

   #+BEGIN_SRC
     #"enable_cfengine_enterprise_hub_ha" expression => "enterprise_edition";
   #+END_SRC

6) Trigger the policy update *on both nodes*.

   #+BEGIN_SRC shell
     cf-agent -Kf update.cf
   #+END_SRC

7) Restart CFEngine (may not be needed?).

   #+BEGIN_SRC shell
     service cfengine3 restart
   #+END_SRC
